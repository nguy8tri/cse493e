Website/App Assessment Reflection
=========

### Accessibility Technology Familiarity Competency
#### AT #1: Screen Reader
I used the screen reader that was made available as part of Samsung's Accessibility Suite.
1. It is a standard screen reader which essentially allows non-sighted or low-vision persons to navigate a phone by having the phone read out the contents of the screen to them.
2. It supports those who are non-sighted or have low-vision, just visual access needs in general.
3. The gestures that it implements to read out text is very intuitive. As a sighted person, it was very easy to use, and had ways of distinguishing gestures to read a particular part of a phone v. pressing a button or scrolling, which was very good. However, I am in doubt as to how a visual access needs person would be able to press a button, which requires them to press the same button twice, which could be hard if you can't see the button.
4. When I tried it, I found myself initially struggling to figure out how to scroll and select buttons instead of having the phone read things out to me. However, as I experimented, I quickly figured out how to do those actions, and after those initial hurdles (as well as others), I found using the screen reader okay. It was still somewhat bulky because of the extra gestures you have to do compared to normal.
#### AT #2: Voice Access
I used Voice Access, which is also part of Samsung's Accessibility Suite.
1. It is a voice input device that allows a person to navigate their phone with voice commands.
2. This supports people with dexterity access needs.
3. Because it interacts with vocal commands, most commands come very easy. Things like "start typing", "press button A" are very intuitive. When a user is trying to access a particular feature and they don't know how, they can simply say "help", and it will give a list of commands to help accomplish things they want to do with the phone. However, it isn't obvious sometimes as to how a user should access particular buttons or interactive features. In this case, you must call out the number that the Voice Access App assigns to the feature, which you access via "show labels".
4. When I tried it, I adjusted to it far easier than the Screen Reader. It was super intuitive to use, and when I wasn't sure of how to access a feature, I could always just say "help", and I would be able to figure it out from there.
### Automated Checking Competency
I used Google's Accessibility Checker on my Samsung phone. I really liked how easy it was to use. Once activated, this blue check mark is always present on your screen and anytime you want to check the accessibility of something, you simply press it, and press "screenshot" to have it evaluate just a single screen, or "record" to have it evaluate a whole session of you using an app. It also allows you to access all accessibility features, like the two above, through that same blue check mark.

In respect to the POUR guidelines, since I was using a phone and I did not employ a switch controller, I did not use a keyboard in that sense. However, I did make sure that using any text field with the virtual keyboard was fairly easy. Time was not a factor in this app, as the features I tried were not time sensitive. To test ease of navigation and pointer use, I used the "record" feature of this AT to essentially go through the app to do typical things with the app, and then would stop the recording, which would give me an accessibility report across all the things I used in that particular session. I tested Predictability also using this mechanism, and though it isn't really included in this report, being able to record the use of a page over its history was particularly useful in uncovering things like obstruction of text via pop-ups. Readability would be testable simply by having this AT take a screenshot of any particular page in most cases.

This AT failed to catch any issues over Input Assistance, particularly error checking, as I could freely change the amenities a Park had without any confirmation or any validation, and it would not say anything about it.
### Accessibility Rules Competency
I assessed the following tasks:
1. Logging into the App
2. Using the search bar to navigate to a location
3. Selecting Parks and viewing their pages
4. Leaving a Review and Updating Park descriptions
### Use of GAI
I did use GAI. Within the second UAR (AT-Google Accessibility Scanner-2), I reference chatgpt, which I used to check whether some very small buttons I found are within WCAG regulation. Because the AT I used gave me units for the button that were not in terms of the one referenced within WCAG regulation 2.5.5, I search the internet to see if I could convert between the two, and after failing for about 10 to 15 minutes, I consulted chatgpt.